{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7933399,"sourceType":"datasetVersion","datasetId":4663399},{"sourceId":7933661,"sourceType":"datasetVersion","datasetId":4663548},{"sourceId":7933845,"sourceType":"datasetVersion","datasetId":4663676},{"sourceId":7943206,"sourceType":"datasetVersion","datasetId":4670241},{"sourceId":7944450,"sourceType":"datasetVersion","datasetId":4670695}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n# ALL\nimport keras\nimport os\nimport random\n\nimport os\n# AS\n# %matplotlib inline\n# %matplotlib notebook\nimport matplotlib.pyplot as plt\n# %matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\n\n# FROM\nfrom concurrent.futures import ProcessPoolExecutor\nfrom contextlib import redirect_stdout\nfrom keras.models import Sequential\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom PIL import Image\nfrom PIL import UnidentifiedImageError\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, f1_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.layers import Input, Dense, Flatten, Conv3D, MaxPooling3D, GlobalAveragePooling3D, concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\n\n\n###########\nimport os\nimport sys\nimport torch\n\n# AS\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# FROM\nfrom PIL import Image\nfrom PIL import UnidentifiedImageError","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-03-26T07:27:57.257914Z","iopub.execute_input":"2024-03-26T07:27:57.258244Z","iopub.status.idle":"2024-03-26T07:28:03.641594Z","shell.execute_reply.started":"2024-03-26T07:27:57.258210Z","shell.execute_reply":"2024-03-26T07:28:03.640508Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-03-26 07:27:57.629598: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-03-26 07:27:57.629655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-03-26 07:27:57.631257: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# tf.keras.backend.clear_session()\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.647938Z","iopub.execute_input":"2024-03-26T07:28:03.648188Z","iopub.status.idle":"2024-03-26T07:28:03.701292Z","shell.execute_reply.started":"2024-03-26T07:28:03.648165Z","shell.execute_reply":"2024-03-26T07:28:03.700273Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.15.0\nNum GPUs Available:  2\n","output_type":"stream"}]},{"cell_type":"code","source":"# ##### pytorch version and GPU\n# print(\"PyTorch version:\", torch.__version__)\n# print(\"MPS Available: \", torch.backends.mps.is_available()) # torch.cuda.is_available()\n# # print(\"MPS Version: \", torch.backends.mps.__version__)\n# print(\"Number of GPUs: \", torch.cuda.device_count())\n# # print(\"Current GPU Device: \", torch.backends.mps.current_device())\n\n# # activate the mps\n# # torch.backends.mps.enable_non_blocking()\n# # torch.backends.mps.current_device()\n\n# # Print available devices, MPS should be among them\n# print(torch.backends.mps.is_available())\n\n# # Set device to MPS\n# device = torch.device(\"mps\")\n# print(f\"Using device: {device}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.702869Z","iopub.execute_input":"2024-03-26T07:28:03.703271Z","iopub.status.idle":"2024-03-26T07:28:03.718473Z","shell.execute_reply.started":"2024-03-26T07:28:03.703237Z","shell.execute_reply":"2024-03-26T07:28:03.717566Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# import torch\n# torch.cuda.empty_cache()\n# torch.cuda.memory_summary(device=None, abbreviated=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.721219Z","iopub.execute_input":"2024-03-26T07:28:03.721521Z","iopub.status.idle":"2024-03-26T07:28:03.728121Z","shell.execute_reply.started":"2024-03-26T07:28:03.721496Z","shell.execute_reply":"2024-03-26T07:28:03.727259Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Loading Data\nmain_root_directory = \"/Users/izzymohamed/Desktop/MLPData\"\nMRI_modal_directory = \"/kaggle/input/mriscans\"\nCT_modal_directory = \"/kaggle/input/ctscan/CT\"\npet_modal_directory = \"/kaggle/input/petscans\"\npreload_folder = \"/kaggle/input/samplepreprocesseddata\"\nresults_folder = \"/kaggle/working\"","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.729178Z","iopub.execute_input":"2024-03-26T07:28:03.729456Z","iopub.status.idle":"2024-03-26T07:28:03.739103Z","shell.execute_reply.started":"2024-03-26T07:28:03.729428Z","shell.execute_reply":"2024-03-26T07:28:03.738327Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Create new folder to save the results of the model\nMRI_folder_name = MRI_modal_directory.split('/')[-2] + \"/\" + MRI_modal_directory.split('/')[-1]\nCT_folder_name = CT_modal_directory.split('/')[-2] + \"/\" + CT_modal_directory.split('/')[-1]\nPET_folder_name = pet_modal_directory.split('/')[-2] + \"/\" + pet_modal_directory.split('/')[-1]","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.740127Z","iopub.execute_input":"2024-03-26T07:28:03.740384Z","iopub.status.idle":"2024-03-26T07:28:03.749682Z","shell.execute_reply.started":"2024-03-26T07:28:03.740361Z","shell.execute_reply":"2024-03-26T07:28:03.748771Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def collect_files_from_directories(main_root_directory):\n    categories = [\n        \"non-demented\",\n        \"very-mild-dementia\",\n        \"mild-dementia\",\n        \"moderate-dementia\",\n        \"unknown\",\n    ]\n    \n    # Initialize an empty list to collect file info\n    collected_files = []\n\n    # Walk through the directory structure\n    for root, dirs, files in os.walk(main_root_directory):\n        # Check if the current folder is one of the categories\n        category = os.path.basename(root)\n        if category in categories:\n            print(category)\n            # Construct the full file paths in one go and append them\n            collected_files.extend([(os.path.join(root, file), category) for file in files])\n                \n    print(\"DONE\")\n    \n    # Convert the collected file info into a DataFrame\n    df = pd.DataFrame(collected_files, columns=['FilePath', 'Category'])\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.750607Z","iopub.execute_input":"2024-03-26T07:28:03.750896Z","iopub.status.idle":"2024-03-26T07:28:03.760918Z","shell.execute_reply.started":"2024-03-26T07:28:03.750865Z","shell.execute_reply":"2024-03-26T07:28:03.759920Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# collect_files_from_directories_df_mri = collect_files_from_directories(MRI_modal_directory)\n# collect_files_from_directories_df_ct = collect_files_from_directories(CT_modal_directory)\n# collect_files_from_directories_df_pet = collect_files_from_directories(pet_modal_directory)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.762043Z","iopub.execute_input":"2024-03-26T07:28:03.762505Z","iopub.status.idle":"2024-03-26T07:28:03.774545Z","shell.execute_reply.started":"2024-03-26T07:28:03.762473Z","shell.execute_reply":"2024-03-26T07:28:03.773754Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# convert collect_files_from_directories_df to dictionary where Category is the key and the value is the list of file paths\ndef convert_df_to_dict(df):\n    # Initialize an empty dictionary\n    file_dict = {}\n    categories = [\n        \"non-demented\",\n        \"very-mild-dementia\",\n        \"mild-dementia\",\n        \"moderate-dementia\",\n        \"unknown\",\n    ]\n    # Loop through the unique categories\n    for category in categories:\n        # Collect the file paths for the current category\n        file_dict[category] = df[df['Category'] == category]['FilePath'].values\n    \n    return file_dict","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.775674Z","iopub.execute_input":"2024-03-26T07:28:03.776051Z","iopub.status.idle":"2024-03-26T07:28:03.785677Z","shell.execute_reply.started":"2024-03-26T07:28:03.776005Z","shell.execute_reply":"2024-03-26T07:28:03.784855Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"images_dict_mri = [] # convert_df_to_dict(collect_files_from_directories_df_mri)\nimages_dict_ct = [] # convert_df_to_dict(collect_files_from_directories_df_ct)\nimages_dict_pet = [] # convert_df_to_dict(collect_files_from_directories_df_pet)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.788466Z","iopub.execute_input":"2024-03-26T07:28:03.788963Z","iopub.status.idle":"2024-03-26T07:28:03.796525Z","shell.execute_reply.started":"2024-03-26T07:28:03.788938Z","shell.execute_reply":"2024-03-26T07:28:03.795720Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Now you can access the images using the keys in images_dict_mri and images_dict_ct\nnon_demented_mri = images_dict_mri['non-demented'] if 'non-demented' in images_dict_mri else []\nvery_mild_demented_mri = images_dict_mri['very-mild-dementia'] if 'very-mild-dementia' in images_dict_mri else []\nmild_demented_mri = images_dict_mri['mild-dementia'] if 'mild-dementia' in images_dict_mri else []\nmoderate_demented_mri = images_dict_mri['moderate-dementia'] if 'moderate-dementia' in images_dict_mri else []\nunknown_mri = images_dict_mri['unknown'] if 'unknown' in images_dict_mri else []","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.797602Z","iopub.execute_input":"2024-03-26T07:28:03.797975Z","iopub.status.idle":"2024-03-26T07:28:03.811072Z","shell.execute_reply.started":"2024-03-26T07:28:03.797952Z","shell.execute_reply":"2024-03-26T07:28:03.810036Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"non_demented_ct = images_dict_ct['non-demented'] if 'non-demented' in images_dict_ct else []\nvery_mild_demented_ct = images_dict_ct['very-mild-dementia'] if 'very-mild-dementia' in images_dict_ct else []\nmild_demented_ct = images_dict_ct['mild-dementia'] if 'mild-dementia' in images_dict_ct else []\nmoderate_demented_ct = images_dict_ct['moderate-dementia'] if 'moderate-dementia' in images_dict_ct else []\nunknown_ct = images_dict_ct['unknown'] if 'unknown' in images_dict_ct else []","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.812354Z","iopub.execute_input":"2024-03-26T07:28:03.812644Z","iopub.status.idle":"2024-03-26T07:28:03.821491Z","shell.execute_reply.started":"2024-03-26T07:28:03.812620Z","shell.execute_reply":"2024-03-26T07:28:03.820571Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# ADD the pet data\nnon_demented_pet = images_dict_pet['non-demented'] if 'non-demented' in images_dict_pet else []\nvery_mild_demented_pet = images_dict_pet['very-mild-dementia'] if 'very-mild-dementia' in images_dict_pet else []\nmild_demented_pet = images_dict_pet['mild-dementia'] if 'mild-dementia' in images_dict_pet else []\nmoderate_demented_pet = images_dict_pet['moderate-dementia'] if 'moderate-dementia' in images_dict_pet else []","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.826440Z","iopub.execute_input":"2024-03-26T07:28:03.827013Z","iopub.status.idle":"2024-03-26T07:28:03.832117Z","shell.execute_reply.started":"2024-03-26T07:28:03.826988Z","shell.execute_reply":"2024-03-26T07:28:03.831152Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# If you need to print the count for each category:\n# for key, value in images_dict_mri.items():\n#     print(f\"{key} (MRI): {len(value)} images\")\n\n# for key, value in images_dict_ct.items():\n#     print(f\"{key} (CT): {len(value)} images\")\n\n# for key, value in images_dict_pet.items():\n#     print(f\"{key} (PET): {len(value)} images\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.833118Z","iopub.execute_input":"2024-03-26T07:28:03.833428Z","iopub.status.idle":"2024-03-26T07:28:03.843253Z","shell.execute_reply.started":"2024-03-26T07:28:03.833404Z","shell.execute_reply":"2024-03-26T07:28:03.842324Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(f\"Non-demented: {len(non_demented_mri)}\")\nprint(f\"Very mild dementia: {len(very_mild_demented_mri)}\")\nprint(f\"Mild dementia: {len(mild_demented_mri)}\")\nprint(f\"Moderate dementia: {len(moderate_demented_mri)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.844315Z","iopub.execute_input":"2024-03-26T07:28:03.844653Z","iopub.status.idle":"2024-03-26T07:28:03.858183Z","shell.execute_reply.started":"2024-03-26T07:28:03.844621Z","shell.execute_reply":"2024-03-26T07:28:03.857063Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Non-demented: 0\nVery mild dementia: 0\nMild dementia: 0\nModerate dementia: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Non-demented: {len(non_demented_ct)}\")\nprint(f\"Very mild dementia: {len(very_mild_demented_ct)}\")\nprint(f\"Mild dementia: {len(mild_demented_ct)}\")\nprint(f\"Moderate dementia: {len(moderate_demented_ct)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.859337Z","iopub.execute_input":"2024-03-26T07:28:03.859676Z","iopub.status.idle":"2024-03-26T07:28:03.868582Z","shell.execute_reply.started":"2024-03-26T07:28:03.859641Z","shell.execute_reply":"2024-03-26T07:28:03.867639Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Non-demented: 0\nVery mild dementia: 0\nMild dementia: 0\nModerate dementia: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Non-demented: {len(non_demented_pet)}\")\nprint(f\"Very mild dementia: {len(very_mild_demented_pet)}\")\nprint(f\"Mild dementia: {len(mild_demented_pet)}\")\nprint(f\"Moderate dementia: {len(moderate_demented_pet)}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.869854Z","iopub.execute_input":"2024-03-26T07:28:03.870203Z","iopub.status.idle":"2024-03-26T07:28:03.878021Z","shell.execute_reply.started":"2024-03-26T07:28:03.870179Z","shell.execute_reply":"2024-03-26T07:28:03.877128Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Non-demented: 0\nVery mild dementia: 0\nMild dementia: 0\nModerate dementia: 0\n","output_type":"stream"}]},{"cell_type":"code","source":"def display_images_with_text(images, category_name,):\n    plt.figure(figsize=(15, 5))\n    plt.suptitle(f\"Images from {category_name}\", fontsize=16)\n\n    display_count = 0 # Initialize a counter for displayed images\n\n    if len(images) == 0:\n        print(\"No images to display\")\n        return\n    \n    # Ensure to display up to 3 images only\n    for i in range(min(3, len(images))):\n        if display_count < 3: # Check if less than 3 images have been displayed\n            img_path = images[i]\n            img = Image.open(img_path)\n            plt.subplot(1, 3, display_count + 1) # Use display_count for subplot indexing\n            plt.imshow(img)\n            plt.axis('off')\n            \n            # Add text indicating the category\n            plt.text(0, -10, f\"{category_name.split()[0]} {display_count + 1}\", color='white', fontsize=12, weight='bold', ha='left', va='bottom', bbox=dict(facecolor='black', alpha=0.7))\n            \n            display_count += 1 # Increment the displayed images counter\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.879165Z","iopub.execute_input":"2024-03-26T07:28:03.879441Z","iopub.status.idle":"2024-03-26T07:28:03.888664Z","shell.execute_reply.started":"2024-03-26T07:28:03.879418Z","shell.execute_reply":"2024-03-26T07:28:03.887791Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Display images with text for each category\n# display_images_with_text(non_demented_mri, \"Non Demented\")\n# display_images_with_text(very_mild_demented_mri, \"Very Mild Demented\")\n# display_images_with_text(mild_demented_mri, \"Mild Demented\")\n# display_images_with_text(moderate_demented_mri, \"Moderate Demented\")\n# display_images_with_text(non_demented_ct, \"Non Demented\")\n# display_images_with_text(very_mild_demented_ct, \"Very Mild Demented\")\n# display_images_with_text(mild_demented_ct, \"Mild Demented\")\n# display_images_with_text(moderate_demented_ct, \"Moderate Demented\")\n# display_images_with_text(non_demented_pet, \"Non Demented\")\n# display_images_with_text(very_mild_demented_pet, \"Very Mild Demented\")\n# display_images_with_text(mild_demented_pet, \"Mild Demented\")\n# display_images_with_text(moderate_demented_pet, \"Moderate Demented\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.889642Z","iopub.execute_input":"2024-03-26T07:28:03.889914Z","iopub.status.idle":"2024-03-26T07:28:03.898663Z","shell.execute_reply.started":"2024-03-26T07:28:03.889891Z","shell.execute_reply":"2024-03-26T07:28:03.897794Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Data Preprocessing\n# Set seed for random sampling\nrandom.seed(42)\n\ndef train_test_split_fun(class_df):\n    if len(class_df) > 0:\n        return train_test_split(class_df, test_size=0.2, random_state=42)\n    else:\n        return [], []","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.899694Z","iopub.execute_input":"2024-03-26T07:28:03.900006Z","iopub.status.idle":"2024-03-26T07:28:03.912332Z","shell.execute_reply.started":"2024-03-26T07:28:03.899982Z","shell.execute_reply":"2024-03-26T07:28:03.911408Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Split off a test set for the moderate_demented class\nmoderate_demented_train_mri, moderate_demented_test_mri = train_test_split_fun(moderate_demented_mri)\nmoderate_demented_train_ct, moderate_demented_test_ct = train_test_split_fun(moderate_demented_ct)\nmoderate_demented_train_pet, moderate_demented_test_pet = train_test_split_fun(moderate_demented_pet)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.913655Z","iopub.execute_input":"2024-03-26T07:28:03.914097Z","iopub.status.idle":"2024-03-26T07:28:03.922002Z","shell.execute_reply.started":"2024-03-26T07:28:03.914064Z","shell.execute_reply":"2024-03-26T07:28:03.921240Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Split off a test set for the mild_demented class\nmild_demented_train_mri, mild_demented_test_mri = train_test_split_fun(mild_demented_mri)\nmild_demented_train_ct, mild_demented_test_ct = train_test_split_fun(mild_demented_ct)\nmild_demented_train_pet, mild_demented_test_pet = train_test_split_fun(mild_demented_pet)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.923047Z","iopub.execute_input":"2024-03-26T07:28:03.923334Z","iopub.status.idle":"2024-03-26T07:28:03.932662Z","shell.execute_reply.started":"2024-03-26T07:28:03.923309Z","shell.execute_reply":"2024-03-26T07:28:03.931923Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Split off a test set for the very_mild_demented class\nvery_mild_demented_train_mri, very_mild_demented_test_mri = train_test_split_fun(very_mild_demented_mri)\nvery_mild_demented_train_ct, very_mild_demented_test_ct = train_test_split_fun(very_mild_demented_ct)\nvery_mild_demented_train_pet, very_mild_demented_test_pet = train_test_split_fun(very_mild_demented_pet)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.933801Z","iopub.execute_input":"2024-03-26T07:28:03.934144Z","iopub.status.idle":"2024-03-26T07:28:03.943728Z","shell.execute_reply.started":"2024-03-26T07:28:03.934113Z","shell.execute_reply":"2024-03-26T07:28:03.942975Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Split off a test set for the non_demented class\nnon_demented_train_mri, non_demented_test_mri = train_test_split_fun(non_demented_mri)\nnon_demented_train_ct, non_demented_test_ct = train_test_split_fun(non_demented_ct)\nnon_demented_train_pet, non_demented_test_pet = train_test_split_fun(non_demented_pet)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.944906Z","iopub.execute_input":"2024-03-26T07:28:03.945189Z","iopub.status.idle":"2024-03-26T07:28:03.957640Z","shell.execute_reply.started":"2024-03-26T07:28:03.945167Z","shell.execute_reply":"2024-03-26T07:28:03.956878Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Check number of train images per category\nprint(len(non_demented_train_mri))\nprint(len(very_mild_demented_train_mri))\nprint(len(mild_demented_train_mri))\nprint(len(moderate_demented_train_mri))\nprint(len(non_demented_train_ct))\nprint(len(very_mild_demented_train_ct))\nprint(len(mild_demented_train_ct))\nprint(len(moderate_demented_train_ct))\nprint(len(non_demented_train_pet))\nprint(len(very_mild_demented_train_pet))\nprint(len(mild_demented_train_pet))\nprint(len(moderate_demented_train_pet))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.958786Z","iopub.execute_input":"2024-03-26T07:28:03.959124Z","iopub.status.idle":"2024-03-26T07:28:03.969191Z","shell.execute_reply.started":"2024-03-26T07:28:03.959101Z","shell.execute_reply":"2024-03-26T07:28:03.968314Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"def random_choices_sample(dataset, target_samples):\n    if len(dataset) == 0:\n        # return a list of zeros if the dataset is empty\n        return [0] * target_samples\n    else:\n        if len(dataset) < target_samples:\n            # Oversample small classes\n            return random.choices(dataset, k=target_samples)\n        else: \n            # Undersample large classes\n            return random.sample(list(dataset), k=target_samples)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.970401Z","iopub.execute_input":"2024-03-26T07:28:03.970662Z","iopub.status.idle":"2024-03-26T07:28:03.978917Z","shell.execute_reply.started":"2024-03-26T07:28:03.970640Z","shell.execute_reply":"2024-03-26T07:28:03.978157Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Specify the target number of samples for each class\ntrain_target_samples = 5000","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.979908Z","iopub.execute_input":"2024-03-26T07:28:03.980157Z","iopub.status.idle":"2024-03-26T07:28:03.989450Z","shell.execute_reply.started":"2024-03-26T07:28:03.980137Z","shell.execute_reply":"2024-03-26T07:28:03.988658Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Oversample small classes\nmoderate_demented_samp_mri = random_choices_sample(moderate_demented_train_mri, train_target_samples)\nmild_demented_samp_mri = random_choices_sample(mild_demented_train_mri, train_target_samples)\nvery_mild_demented_samp_mri = random_choices_sample(very_mild_demented_train_mri, train_target_samples)\nnon_demented_samp_mri = random_choices_sample(non_demented_train_mri, train_target_samples)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:03.990666Z","iopub.execute_input":"2024-03-26T07:28:03.991249Z","iopub.status.idle":"2024-03-26T07:28:04.000044Z","shell.execute_reply.started":"2024-03-26T07:28:03.991217Z","shell.execute_reply":"2024-03-26T07:28:03.999201Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# same for the ct\nmoderate_demented_samp_ct = random_choices_sample(moderate_demented_train_ct, train_target_samples)\nmild_demented_samp_ct = random_choices_sample(mild_demented_train_ct, train_target_samples)\nvery_mild_demented_samp_ct = random_choices_sample(very_mild_demented_train_ct, train_target_samples)\nnon_demented_samp_ct = random_choices_sample(non_demented_train_ct, train_target_samples)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.001140Z","iopub.execute_input":"2024-03-26T07:28:04.001876Z","iopub.status.idle":"2024-03-26T07:28:04.014376Z","shell.execute_reply.started":"2024-03-26T07:28:04.001842Z","shell.execute_reply":"2024-03-26T07:28:04.013469Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#same for the pet\nmoderate_demented_samp_pet = random_choices_sample(moderate_demented_train_pet, train_target_samples)\nmild_demented_samp_pet = random_choices_sample(mild_demented_train_pet, train_target_samples)\nvery_mild_demented_samp_pet = random_choices_sample(very_mild_demented_train_pet, train_target_samples)\nnon_demented_samp_pet = random_choices_sample(non_demented_train_pet, train_target_samples)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.015669Z","iopub.execute_input":"2024-03-26T07:28:04.016318Z","iopub.status.idle":"2024-03-26T07:28:04.024578Z","shell.execute_reply.started":"2024-03-26T07:28:04.016287Z","shell.execute_reply":"2024-03-26T07:28:04.023541Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Check results\nprint(len(non_demented_samp_mri))\nprint(len(non_demented_samp_ct))\nprint(len(very_mild_demented_samp_mri))\nprint(len(very_mild_demented_samp_ct))\nprint(len(mild_demented_samp_mri))\nprint(len(mild_demented_samp_ct))\nprint(len(moderate_demented_samp_mri))\nprint(len(moderate_demented_samp_ct))\nprint(len(non_demented_samp_pet))\nprint(len(very_mild_demented_samp_pet))\nprint(len(mild_demented_samp_pet))\nprint(len(moderate_demented_samp_pet))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.025711Z","iopub.execute_input":"2024-03-26T07:28:04.026010Z","iopub.status.idle":"2024-03-26T07:28:04.035135Z","shell.execute_reply.started":"2024-03-26T07:28:04.025987Z","shell.execute_reply":"2024-03-26T07:28:04.034144Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n5000\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check the number of test images per category\nprint(len(non_demented_test_mri))\nprint(len(very_mild_demented_test_mri))\nprint(len(mild_demented_test_mri))\nprint(len(moderate_demented_test_mri))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.036186Z","iopub.execute_input":"2024-03-26T07:28:04.036678Z","iopub.status.idle":"2024-03-26T07:28:04.045066Z","shell.execute_reply.started":"2024-03-26T07:28:04.036650Z","shell.execute_reply":"2024-03-26T07:28:04.043980Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"0\n0\n0\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"# ct\nprint(len(non_demented_test_ct))\nprint(len(very_mild_demented_test_ct))\nprint(len(mild_demented_test_ct))\nprint(len(moderate_demented_test_ct))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.046250Z","iopub.execute_input":"2024-03-26T07:28:04.046585Z","iopub.status.idle":"2024-03-26T07:28:04.056678Z","shell.execute_reply.started":"2024-03-26T07:28:04.046554Z","shell.execute_reply":"2024-03-26T07:28:04.055870Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"0\n0\n0\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"# pet\nprint(len(non_demented_test_pet))\nprint(len(very_mild_demented_test_pet))\nprint(len(mild_demented_test_pet))\nprint(len(moderate_demented_test_pet))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.057733Z","iopub.execute_input":"2024-03-26T07:28:04.058048Z","iopub.status.idle":"2024-03-26T07:28:04.071943Z","shell.execute_reply.started":"2024-03-26T07:28:04.058017Z","shell.execute_reply":"2024-03-26T07:28:04.070924Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"0\n0\n0\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Specify the target number of samples for each class\ntest_target_samples = 500","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.073226Z","iopub.execute_input":"2024-03-26T07:28:04.073486Z","iopub.status.idle":"2024-03-26T07:28:04.081860Z","shell.execute_reply.started":"2024-03-26T07:28:04.073464Z","shell.execute_reply":"2024-03-26T07:28:04.080966Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Undersample large classes\nmild_demented_test_mri = random_choices_sample(mild_demented_test_mri, test_target_samples)\nmild_demented_test_ct = random_choices_sample(mild_demented_test_ct, test_target_samples)\nmild_demented_test_pet = random_choices_sample(mild_demented_test_pet, test_target_samples)\nvery_mild_demented_test_mri = random_choices_sample(very_mild_demented_test_mri, test_target_samples)\nvery_mild_demented_test_ct = random_choices_sample(very_mild_demented_test_ct, test_target_samples)\nvery_mild_demented_test_pet = random_choices_sample(very_mild_demented_test_pet, test_target_samples)\nnon_demented_test_mri = random_choices_sample(non_demented_test_mri, test_target_samples)\nnon_demented_test_ct = random_choices_sample(non_demented_test_ct, test_target_samples)\nnon_demented_test_pet = random_choices_sample(non_demented_test_pet, test_target_samples)\nmoderate_demented_test_mri = random_choices_sample(moderate_demented_test_mri, test_target_samples)\nmoderate_demented_test_ct = random_choices_sample(moderate_demented_test_ct, test_target_samples)\nmoderate_demented_test_pet = random_choices_sample(moderate_demented_test_pet, test_target_samples)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.082792Z","iopub.execute_input":"2024-03-26T07:28:04.083060Z","iopub.status.idle":"2024-03-26T07:28:04.092558Z","shell.execute_reply.started":"2024-03-26T07:28:04.083037Z","shell.execute_reply":"2024-03-26T07:28:04.091793Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# print the results from the test set\nprint(len(non_demented_test_mri))\nprint(len(very_mild_demented_test_mri))\nprint(len(mild_demented_test_mri))\nprint(len(moderate_demented_test_mri))\nprint(len(non_demented_test_ct))\nprint(len(very_mild_demented_test_ct))\nprint(len(mild_demented_test_ct))\nprint(len(moderate_demented_test_ct))\nprint(len(non_demented_test_pet))\nprint(len(very_mild_demented_test_pet))\nprint(len(mild_demented_test_pet))\nprint(len(moderate_demented_test_pet))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.093654Z","iopub.execute_input":"2024-03-26T07:28:04.094004Z","iopub.status.idle":"2024-03-26T07:28:04.105024Z","shell.execute_reply.started":"2024-03-26T07:28:04.093973Z","shell.execute_reply":"2024-03-26T07:28:04.103995Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n500\n","output_type":"stream"}]},{"cell_type":"code","source":"# One-hot encoder for class labels \nencoder = OneHotEncoder()\nencoder.fit([[0],[1],[2],[3]])","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.106446Z","iopub.execute_input":"2024-03-26T07:28:04.107208Z","iopub.status.idle":"2024-03-26T07:28:04.122861Z","shell.execute_reply.started":"2024-03-26T07:28:04.107174Z","shell.execute_reply":"2024-03-26T07:28:04.121771Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"OneHotEncoder()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# merge the mri and ct data where mri is the first item and ct is the second item\nnon_demented_samp = list(zip(non_demented_samp_mri, non_demented_samp_ct, non_demented_samp_pet))\nvery_mild_demented_samp = list(zip(very_mild_demented_samp_mri, very_mild_demented_samp_ct, very_mild_demented_samp_pet))\nmild_demented_samp = list(zip(mild_demented_samp_mri, mild_demented_samp_ct, mild_demented_samp_pet))\nmoderate_demented_samp = list(zip(moderate_demented_samp_mri, moderate_demented_samp_ct, moderate_demented_samp_pet))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.123929Z","iopub.execute_input":"2024-03-26T07:28:04.124253Z","iopub.status.idle":"2024-03-26T07:28:04.134449Z","shell.execute_reply.started":"2024-03-26T07:28:04.124230Z","shell.execute_reply":"2024-03-26T07:28:04.133711Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# merge the test sets\nnon_demented_test = list(zip(non_demented_test_mri, non_demented_test_ct, non_demented_test_pet))\nvery_mild_demented_test = list(zip(very_mild_demented_test_mri, very_mild_demented_test_ct, very_mild_demented_test_pet))\nmild_demented_test = list(zip(mild_demented_test_mri, mild_demented_test_ct, mild_demented_test_pet))\nmoderate_demented_test = list(zip(moderate_demented_test_mri, moderate_demented_test_ct, moderate_demented_test_pet))","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.145439Z","iopub.execute_input":"2024-03-26T07:28:04.145695Z","iopub.status.idle":"2024-03-26T07:28:04.151002Z","shell.execute_reply.started":"2024-03-26T07:28:04.145674Z","shell.execute_reply":"2024-03-26T07:28:04.150155Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Mapping categories to their labels\ncategories = {\n    'non_demented_samp': 0,\n    'very_mild_demented_samp': 1,\n    'mild_demented_samp': 2,\n    'moderate_demented_samp': 3\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.152112Z","iopub.execute_input":"2024-03-26T07:28:04.152367Z","iopub.status.idle":"2024-03-26T07:28:04.160921Z","shell.execute_reply.started":"2024-03-26T07:28:04.152345Z","shell.execute_reply":"2024-03-26T07:28:04.160020Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"categories","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.161990Z","iopub.execute_input":"2024-03-26T07:28:04.162316Z","iopub.status.idle":"2024-03-26T07:28:04.173163Z","shell.execute_reply.started":"2024-03-26T07:28:04.162286Z","shell.execute_reply":"2024-03-26T07:28:04.172162Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"{'non_demented_samp': 0,\n 'very_mild_demented_samp': 1,\n 'mild_demented_samp': 2,\n 'moderate_demented_samp': 3}"},"metadata":{}}]},{"cell_type":"code","source":"# import numpy as np\n# from PIL import Image, UnidentifiedImageError\n\n# data = []\n# result = []\n\n# for category, label in categories.items():\n#     print(f\"Processing category: {category}\")\n    \n#     paths_list = globals().get(category, [])  # Fetch the paths for the current category\n#     total_paths = len(paths_list)  # Total number of paths for the current category\n#     processed_paths = 0  # Initialize the counter for processed paths\n    \n#     for paths in paths_list:\n#         x = 0\n#         sample = []\n#         for i, path in enumerate(paths):\n#             if path == 0:\n#                 img_array = np.zeros((128, 128, 3))\n#                 sample.append(img_array)\n#             else:\n#                 try:\n#                     img = Image.open(path)\n#                 except UnidentifiedImageError:\n#                     print(f\"Unable to identify image file: {path}\")\n#                     x = 1\n#                     break\n#                 img = img.resize((128, 128))\n#                 img_array = np.array(img)[:,:,:3]\n#                 if img_array.shape != (128, 128, 3):\n#                     print(f\"Image shape is not (128, 128, 3): {img_array.shape}\")\n#                     x = 1\n#                     break\n#                 sample.append(img_array)\n        \n#         if x == 0:\n#             data.append(sample)\n#             result.append(label)\n        \n#         processed_paths += 1  # Increment the counter after processing each path\n#         #completion_percentage = (processed_paths / total_paths) * 100\n#         #print(f\"Completed {completion_percentage:.2f}% of {category}\")\n    \n#     print(f\"Completed processing {category}: {len(data)} samples collected\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.174062Z","iopub.execute_input":"2024-03-26T07:28:04.174331Z","iopub.status.idle":"2024-03-26T07:28:04.183577Z","shell.execute_reply.started":"2024-03-26T07:28:04.174308Z","shell.execute_reply":"2024-03-26T07:28:04.182538Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# data = np.array(data)\n# print(data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.184812Z","iopub.execute_input":"2024-03-26T07:28:04.185160Z","iopub.status.idle":"2024-03-26T07:28:04.196872Z","shell.execute_reply.started":"2024-03-26T07:28:04.185129Z","shell.execute_reply":"2024-03-26T07:28:04.196087Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# np.save(results_folder + f'/data_{train_target_samples}.h5', data, allow_pickle=True, fix_imports=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.198282Z","iopub.execute_input":"2024-03-26T07:28:04.198622Z","iopub.status.idle":"2024-03-26T07:28:04.206477Z","shell.execute_reply.started":"2024-03-26T07:28:04.198592Z","shell.execute_reply":"2024-03-26T07:28:04.205661Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"data = np.load(preload_folder + f'/data_{train_target_samples}.h5.npy', allow_pickle=True)\ndata.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:04.207594Z","iopub.execute_input":"2024-03-26T07:28:04.207944Z","iopub.status.idle":"2024-03-26T07:28:05.304668Z","shell.execute_reply.started":"2024-03-26T07:28:04.207913Z","shell.execute_reply":"2024-03-26T07:28:05.303738Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"(20000, 3, 128, 128, 3)"},"metadata":{}}]},{"cell_type":"code","source":"# result = np.array(result)\n# # Reshape to the one-hot encoded format \n# #result = result.reshape((data.shape[0],4)) \n# print(result.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:05.306103Z","iopub.execute_input":"2024-03-26T07:28:05.306955Z","iopub.status.idle":"2024-03-26T07:28:05.311342Z","shell.execute_reply.started":"2024-03-26T07:28:05.306917Z","shell.execute_reply":"2024-03-26T07:28:05.310229Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Save the arrays to .npy files\n# np.save(results_folder + f'/results_{train_target_samples}.h5', result, allow_pickle=True, fix_imports=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:05.312616Z","iopub.execute_input":"2024-03-26T07:28:05.313015Z","iopub.status.idle":"2024-03-26T07:28:05.325001Z","shell.execute_reply.started":"2024-03-26T07:28:05.312978Z","shell.execute_reply":"2024-03-26T07:28:05.324036Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"result = np.load(preload_folder + f'/results_{train_target_samples}.h5.npy', allow_pickle=True)\nresult.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:05.326249Z","iopub.execute_input":"2024-03-26T07:28:05.326621Z","iopub.status.idle":"2024-03-26T07:28:05.338544Z","shell.execute_reply.started":"2024-03-26T07:28:05.326586Z","shell.execute_reply":"2024-03-26T07:28:05.337631Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"(20000,)"},"metadata":{}}]},{"cell_type":"code","source":"# Split in validation and train data \nx_train,x_val,y_train,y_val = train_test_split(data,result, test_size=0.20, shuffle=True, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:05.339669Z","iopub.execute_input":"2024-03-26T07:28:05.339992Z","iopub.status.idle":"2024-03-26T07:28:06.187954Z","shell.execute_reply.started":"2024-03-26T07:28:05.339966Z","shell.execute_reply":"2024-03-26T07:28:06.187131Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Mapping categories to their labels\ntest_categories = {\n    'non_demented_test': 0,\n    'very_mild_demented_test': 1,\n    'mild_demented_test': 2,\n    'moderate_demented_test': 3\n}","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.189158Z","iopub.execute_input":"2024-03-26T07:28:06.189457Z","iopub.status.idle":"2024-03-26T07:28:06.194805Z","shell.execute_reply.started":"2024-03-26T07:28:06.189432Z","shell.execute_reply":"2024-03-26T07:28:06.193787Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# # # import numpy as np\n# from PIL import Image, UnidentifiedImageError\n\n# data_test = []\n# result_test = []\n\n# for category, label in test_categories.items():\n#     print(f\"Processing category: {category}\")\n    \n#     paths_list = globals().get(category, [])  # Fetch the paths for the current category\n#     total_paths = len(paths_list)  # Total number of paths for the current category\n#     processed_paths = 0  # Initialize the counter for processed paths\n    \n#     for paths in paths_list:\n#         x = 0\n#         sample = []\n#         for i, path in enumerate(paths):\n#             if path == 0:\n#                 img_array = np.zeros((128, 128, 3))\n#                 sample.append(img_array)\n#             else:\n#                 try:\n#                     img = Image.open(path)\n#                 except UnidentifiedImageError:\n#                     print(f\"Unable to identify image file: {path}\")\n#                     x = 1\n#                     break\n#                 img = img.resize((128, 128))\n#                 img_array = np.array(img)[:,:,:3]\n#                 if img_array.shape != (128, 128, 3):\n#                     print(f\"Image shape is not (128, 128, 3): {img_array.shape}\")\n#                     x = 1\n#                     break\n#                 sample.append(img_array)\n        \n#         if x == 0:\n#             data_test.append(sample)\n#             result_test.append(label)\n        \n#         processed_paths += 1  # Increment the counter after processing each path\n#         #completion_percentage = (processed_paths / total_paths) * 100\n#         #print(f\"Completed {completion_percentage:.2f}% of {category}\")\n    \n#     print(f\"Completed processing {category}: {len(data_test)} samples collected\")","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.195993Z","iopub.execute_input":"2024-03-26T07:28:06.196356Z","iopub.status.idle":"2024-03-26T07:28:06.206396Z","shell.execute_reply.started":"2024-03-26T07:28:06.196329Z","shell.execute_reply":"2024-03-26T07:28:06.205532Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# Transform data to numpy array\n# data_test = np.array(data_test)\n# data_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.207460Z","iopub.execute_input":"2024-03-26T07:28:06.207777Z","iopub.status.idle":"2024-03-26T07:28:06.221307Z","shell.execute_reply.started":"2024-03-26T07:28:06.207730Z","shell.execute_reply":"2024-03-26T07:28:06.220283Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# np.save(results_folder + f'/data_test_{test_target_samples}.h5', data_test, allow_pickle=True, fix_imports=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.223334Z","iopub.execute_input":"2024-03-26T07:28:06.223717Z","iopub.status.idle":"2024-03-26T07:28:06.229597Z","shell.execute_reply.started":"2024-03-26T07:28:06.223686Z","shell.execute_reply":"2024-03-26T07:28:06.228769Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"data_test = np.load(preload_folder + f'/data_test_{test_target_samples}.h5.npy', allow_pickle=True)\ndata_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.230704Z","iopub.execute_input":"2024-03-26T07:28:06.231023Z","iopub.status.idle":"2024-03-26T07:28:06.344958Z","shell.execute_reply.started":"2024-03-26T07:28:06.231000Z","shell.execute_reply":"2024-03-26T07:28:06.343995Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"(2000, 3, 128, 128, 3)"},"metadata":{}}]},{"cell_type":"code","source":"# Transform labels to numpy array\n# result_test = np.array(result_test)\n# # #result_test = result_test.reshape((data_test.shape[0],4)) \n# result_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.345992Z","iopub.execute_input":"2024-03-26T07:28:06.346281Z","iopub.status.idle":"2024-03-26T07:28:06.350506Z","shell.execute_reply.started":"2024-03-26T07:28:06.346257Z","shell.execute_reply":"2024-03-26T07:28:06.349536Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"# np.save(results_folder + f'/results_test_{test_target_samples}.h5', result_test, allow_pickle=True, fix_imports=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.351859Z","iopub.execute_input":"2024-03-26T07:28:06.352208Z","iopub.status.idle":"2024-03-26T07:28:06.359677Z","shell.execute_reply.started":"2024-03-26T07:28:06.352178Z","shell.execute_reply":"2024-03-26T07:28:06.358682Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"result_test = np.load(preload_folder + f'/results_test_{test_target_samples}.h5.npy', allow_pickle=True)\nresult_test.shape","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.360816Z","iopub.execute_input":"2024-03-26T07:28:06.361284Z","iopub.status.idle":"2024-03-26T07:28:06.372154Z","shell.execute_reply.started":"2024-03-26T07:28:06.361260Z","shell.execute_reply":"2024-03-26T07:28:06.371323Z"},"trusted":true},"execution_count":58,"outputs":[{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"(2000,)"},"metadata":{}}]},{"cell_type":"code","source":"# Change names to x_test and y_test \nx_test = data_test\ny_test = result_test\n#y_test_int = np.argmax(y_test, axis=1)\n# Create the train, validation init data\n#y_train_int = np.argmax(y_train, axis=1)\n#y_val_int = np.argmax(y_val, axis=1)\n# Print the shapes of the train, validation init data\nprint(y_train)\nprint(y_val)\n# Training","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.373236Z","iopub.execute_input":"2024-03-26T07:28:06.374041Z","iopub.status.idle":"2024-03-26T07:28:06.380775Z","shell.execute_reply.started":"2024-03-26T07:28:06.374009Z","shell.execute_reply":"2024-03-26T07:28:06.379874Z"},"trusted":true},"execution_count":59,"outputs":[{"name":"stdout","text":"[1 0 1 ... 1 0 3]\n[2 0 1 ... 1 0 3]\n","output_type":"stream"}]},{"cell_type":"code","source":"class Encoder(nn.Module):\n    def __init__(self, input_channels, output_channels):\n        super(Encoder, self).__init__()\n        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2)\n        self.fc = nn.Linear(128*32*32, output_channels)\n\n    def forward(self, x):\n        # Permute the input tensor from (N, H, W, C) to (N, C, H, W)\n        x = x.permute(2,0,1) # TODO: THis line should fix the error\n        x = F.relu(self.conv1(x))\n        x = self.pool(x)\n        x = F.relu(self.conv2(x))\n        x = self.pool(x)\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x)\n        x = self.fc(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.381962Z","iopub.execute_input":"2024-03-26T07:28:06.382243Z","iopub.status.idle":"2024-03-26T07:28:06.391965Z","shell.execute_reply.started":"2024-03-26T07:28:06.382220Z","shell.execute_reply":"2024-03-26T07:28:06.391025Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass TransformerEncoder(nn.Module):\n    def __init__(self, input_size, num_heads=4, hidden_size=64, num_layers=1, dropout=0.1):\n        super(TransformerEncoder, self).__init__()\n        self.norm = nn.LayerNorm(input_size)\n        self.encoder_layer = nn.TransformerEncoderLayer(d_model=input_size, nhead=num_heads, dim_feedforward=hidden_size, dropout=dropout)\n        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n\n    def forward(self, x):\n        x = self.norm(x)\n        x = self.transformer_encoder(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.393144Z","iopub.execute_input":"2024-03-26T07:28:06.395068Z","iopub.status.idle":"2024-03-26T07:28:06.405445Z","shell.execute_reply.started":"2024-03-26T07:28:06.395041Z","shell.execute_reply":"2024-03-26T07:28:06.404563Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"'''\n# This lead to 25% after 10 epochs\nfrom torch.utils.checkpoint import checkpoint\nclass MultimodalTransformer(nn.Module):\n    def __init__(self, input_channels, output_channels, transformer_input_size, num_classes=4):\n        super(MultimodalTransformer, self).__init__()\n\n        # Create two encoders for processing the two input tensors\n        \n        ###### with one encoder\n        self.encoder= Encoder(input_channels, transformer_input_size)\n        ##### with multiple encoders\n        #self.encoder1 = Encoder(input_channels, transformer_input_size)\n        #self.encoder2 = Encoder(input_channels, transformer_input_size)\n        #self.encoder3 = Encoder(input_channels, transformer_input_size)\n\n        # Transformer encoder for processing the concatenated encoded features\n        self.transformer_encoder = TransformerEncoder(transformer_input_size * 3)  # Doubled input size\n\n        # MLP for classification\n        self.mlp = nn.Sequential(\n            nn.Linear(transformer_input_size * 3, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_classes),\n            nn.Softmax(dim=0)\n\n        )\n\n    def forward(self, inputs):\n        #batch_size = len(inputs)  # Get batch size\n\n        # List to store encoded features\n        encoded_features = []\n\n        for input in inputs:\n            mri, ct, pet = input  \n\n            # Process each modality through its respective encoder\n            encoded1 = self.encoder(mri)\n            encoded2 = self.encoder(ct)\n            encoded3 = self.encoder(pet)\n\n            # Flatten and concatenate (assuming encoders have same output size)\n            combined = torch.cat([encoded1, encoded2, encoded3], dim=0)\n\n            # Append combined features to list\n            encoded_features.append(combined)\n            \n        # Convert list to a tensor for further processing\n        combined_features = torch.stack(encoded_features, dim=0)\n\n        # Pass the combined features through the transformer encoder\n        transformed = self.transformer_encoder(combined_features)\n    \n\n        # Classify using the MLP\n        logits = self.mlp(transformed)\n\n        return logits\n'''","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.406614Z","iopub.execute_input":"2024-03-26T07:28:06.406978Z","iopub.status.idle":"2024-03-26T07:28:06.423016Z","shell.execute_reply.started":"2024-03-26T07:28:06.406946Z","shell.execute_reply":"2024-03-26T07:28:06.422057Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"'\\n# This lead to 25% after 10 epochs\\nfrom torch.utils.checkpoint import checkpoint\\nclass MultimodalTransformer(nn.Module):\\n    def __init__(self, input_channels, output_channels, transformer_input_size, num_classes=4):\\n        super(MultimodalTransformer, self).__init__()\\n\\n        # Create two encoders for processing the two input tensors\\n        \\n        ###### with one encoder\\n        self.encoder= Encoder(input_channels, transformer_input_size)\\n        ##### with multiple encoders\\n        #self.encoder1 = Encoder(input_channels, transformer_input_size)\\n        #self.encoder2 = Encoder(input_channels, transformer_input_size)\\n        #self.encoder3 = Encoder(input_channels, transformer_input_size)\\n\\n        # Transformer encoder for processing the concatenated encoded features\\n        self.transformer_encoder = TransformerEncoder(transformer_input_size * 3)  # Doubled input size\\n\\n        # MLP for classification\\n        self.mlp = nn.Sequential(\\n            nn.Linear(transformer_input_size * 3, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, 32),\\n            nn.ReLU(),\\n            nn.Linear(32, num_classes),\\n            nn.Softmax(dim=0)\\n\\n        )\\n\\n    def forward(self, inputs):\\n        #batch_size = len(inputs)  # Get batch size\\n\\n        # List to store encoded features\\n        encoded_features = []\\n\\n        for input in inputs:\\n            mri, ct, pet = input  \\n\\n            # Process each modality through its respective encoder\\n            encoded1 = self.encoder(mri)\\n            encoded2 = self.encoder(ct)\\n            encoded3 = self.encoder(pet)\\n\\n            # Flatten and concatenate (assuming encoders have same output size)\\n            combined = torch.cat([encoded1, encoded2, encoded3], dim=0)\\n\\n            # Append combined features to list\\n            encoded_features.append(combined)\\n            \\n        # Convert list to a tensor for further processing\\n        combined_features = torch.stack(encoded_features, dim=0)\\n\\n        # Pass the combined features through the transformer encoder\\n        transformed = self.transformer_encoder(combined_features)\\n    \\n\\n        # Classify using the MLP\\n        logits = self.mlp(transformed)\\n\\n        return logits\\n'"},"metadata":{}}]},{"cell_type":"code","source":"\n## Unet Model\n## without the transformer it lead to 90% after 10 epoch \n## with transformer it lead to 25% after 10 epochs\nclass MultimodalTransformer(nn.Module):\n    def __init__(self, input_channels, output_channels, transformer_input_size, num_classes=4):\n        super(MultimodalTransformer, self).__init__()\n\n        # Load the pretrained UNet model\n        unet_model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n                                    in_channels=3, out_channels=1, init_features=32, pretrained=False)\n#         unet_model.eval()  # Set the model to evaluation mode\n        \n        # Freeze the parameters of the UNet model\n#         for param in unet_model.parameters():\n#             param.requires_grad = False\n\n        self.unet = unet_model\n\n        # MLP for transforming encoded value to 1D dimension\n        self.mlp_encode = nn.Sequential(\n            nn.Linear(128*128, transformer_input_size),  # Assuming 256*256 is the size of the encoded value\n            nn.ReLU()\n        )\n\n        # Transformer encoder for processing the concatenated encoded features\n        self.transformer_encoder = TransformerEncoder(transformer_input_size * 3)  # Doubled input size\n\n        # MLP for classification\n        self.mlp_classify = nn.Sequential(\n            nn.Linear(transformer_input_size * 3, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, num_classes),\n            nn.Softmax(dim=1)  # Assuming batch dimension is dim=0\n        )\n\n    def forward(self, inputs):\n        # the compined feature is a list with the length of inputs(0)\n\n        encoded_features = []\n        \n        for input in inputs:\n            mri, ct, pet = input  \n\n            # Process each modality through the UNet encoder\n           \n            mri = mri.unsqueeze(0)\n            ct = ct.unsqueeze(0)\n            pet = pet.unsqueeze(0)\n            mri = mri.permute(0, 3, 1, 2)\n            ct = ct.permute(0, 3, 1, 2)\n            pet = pet.permute(0, 3, 1, 2)\n            encoded1 = self.unet(mri)\n            encoded2 = self.unet(ct)\n            encoded3 = self.unet(pet)\n            # Reshape the encoded value to 1D dimension\n            encoded1 = encoded1.view(encoded1.size(0), -1)\n            encoded2 = encoded2.view(encoded2.size(0), -1)\n            encoded3 = encoded3.view(encoded3.size(0), -1)\n\n            # Pass through MLP to transform to 1D dimension equal to transformer input size\n            encoded1 = self.mlp_encode(encoded1)\n            encoded2 = self.mlp_encode(encoded2)\n            encoded3 = self.mlp_encode(encoded3)\n\n            # concatenate the encoded features\n            encoded = torch.cat((encoded1, encoded2, encoded3), dim=1)\n            \n            # add the encoded features as a new item in the list\n            encoded_features.append(encoded)\n    \n\n        \n        # concatenate all the element in the encoded features along it's 0 axis and print the type of the combined feature\n        encoded_features = torch.cat(encoded_features, dim=0)\n        transformed = self.transformer_encoder(encoded_features)\n        # Classify using the MLP\n        logits = self.mlp_classify(encoded_features)\n\n        return logits\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.424196Z","iopub.execute_input":"2024-03-26T07:28:06.424544Z","iopub.status.idle":"2024-03-26T07:28:06.438971Z","shell.execute_reply.started":"2024-03-26T07:28:06.424512Z","shell.execute_reply":"2024-03-26T07:28:06.437940Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\n#from torchsummary import summary  # For a summary of the model\n\ndef train_model(model, dataloader, num_epochs=10, lr=0.001, device='cuda'):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    model.to(device)\n\n    epoch_losses = []\n    epoch_accuracies = []\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        \n        for inputs, labels in dataloader:\n\n            optimizer.zero_grad()\n            inputs = [input_tensor.to(device) for input_tensor in inputs]\n            labels = labels.to(device)\n            \n            outputs = model(inputs)\n            \n            loss = criterion(outputs, labels.long())\n            \n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            \n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n        epoch_loss = running_loss / len(dataloader)\n        epoch_accuracy = 100 * correct / total\n        epoch_losses.append(epoch_loss)\n        epoch_accuracies.append(epoch_accuracy)\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}, Accuracy: {100 * correct / total:.2f}%')\n        \n        # Flush the GPU memory\n        torch.cuda.empty_cache()\n\n    return epoch_losses, epoch_accuracies","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.440055Z","iopub.execute_input":"2024-03-26T07:28:06.440330Z","iopub.status.idle":"2024-03-26T07:28:06.453330Z","shell.execute_reply.started":"2024-03-26T07:28:06.440308Z","shell.execute_reply":"2024-03-26T07:28:06.452422Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n# Assuming x_train and y_train are numpy arrays or PyTorch tensors\nx_train_tensor = torch.from_numpy(x_train).float()\ny_train_tensor = torch.from_numpy(y_train).float()\n# x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n# y_train_tensor = torch.tensor(y_train, dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:06.454519Z","iopub.execute_input":"2024-03-26T07:28:06.454865Z","iopub.status.idle":"2024-03-26T07:28:09.738058Z","shell.execute_reply.started":"2024-03-26T07:28:06.454840Z","shell.execute_reply":"2024-03-26T07:28:09.737088Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# Create a TensorDataset\ndataset = TensorDataset(x_train_tensor, y_train_tensor)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:09.739418Z","iopub.execute_input":"2024-03-26T07:28:09.739943Z","iopub.status.idle":"2024-03-26T07:28:09.744923Z","shell.execute_reply.started":"2024-03-26T07:28:09.739903Z","shell.execute_reply":"2024-03-26T07:28:09.743820Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Define batch size and whether to shuffle the data\nbatch_size = 32\nshuffle = True","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:09.746238Z","iopub.execute_input":"2024-03-26T07:28:09.746795Z","iopub.status.idle":"2024-03-26T07:28:09.756807Z","shell.execute_reply.started":"2024-03-26T07:28:09.746759Z","shell.execute_reply":"2024-03-26T07:28:09.755713Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"# Create a DataLoader instance\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\nnum_modalities = 3\ninput_channels = 3\nnum_classes = 4\noutuput_channels = 256\ntransformer_input_size = 256","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:09.758216Z","iopub.execute_input":"2024-03-26T07:28:09.758575Z","iopub.status.idle":"2024-03-26T07:28:09.768962Z","shell.execute_reply.started":"2024-03-26T07:28:09.758547Z","shell.execute_reply":"2024-03-26T07:28:09.767984Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":" # Initialize the model\nmodel = MultimodalTransformer(input_channels, outuput_channels, transformer_input_size)\n\n\n    # Define training parameters\nnum_epochs = 10\nlearning_rate = 0.001\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#device = torch.device(\"cpu\")\n\nprint(f\"Training on device: {device}\")\n\n    # Train the model","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:09.770320Z","iopub.execute_input":"2024-03-26T07:28:09.770658Z","iopub.status.idle":"2024-03-26T07:28:10.192488Z","shell.execute_reply.started":"2024-03-26T07:28:09.770632Z","shell.execute_reply":"2024-03-26T07:28:10.191510Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"Training on device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n","output_type":"stream"}]},{"cell_type":"code","source":"### Model Training\nlosses, accs = train_model(model, dataloader, num_epochs=num_epochs, lr=learning_rate, device=device)\nprint(accs)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T07:28:10.193956Z","iopub.execute_input":"2024-03-26T07:28:10.194263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clear the gpu memory\ntorch.backends.mps.release()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # sample from the x_train and y_train and run the model\n# sample = 0\n# x_train_sample = x_train[sample]\n# y_train_sample = y_train[sample]\n# x_train_sample = torch.tensor(x_train_sample, dtype=torch.float32)\n# y_train_sample = torch.tensor(y_train_sample, dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Initialize the model ef __init__(self, input_channels, output_channels, transformer_input_size, num_classes=4)\n# model = MultimodalTransformer(3, 1024, 2048)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Forward pass\n# output = model(x_train_sample)\n\n# print(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Train the model\n# model_pretrained.fit(x_train, y_train_int, epochs=5, batch_size=32, # 5 epochs to reduce computation time\n#                         validation_data=(x_val, y_val_int))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\nfrom torch.utils.data import DataLoader\n# Assuming x_train and y_train are numpy arrays or PyTorch tensors\n\nx_val_tensor = torch.tensor(x_val, dtype=torch.float32)\ny_val_tensor = torch.tensor(y_val, dtype=torch.float32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a TensorDataset\ndataset_val = TensorDataset(x_val_tensor, y_val_tensor)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define batch size and whether to shuffle the data\nbatch_size = 8\nshuffle = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataLoader instance\ndataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=shuffle)\nprint(len(x_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Model Validation\ndef validate_model(model, dataloader, device='cuda'):\n    criterion = nn.CrossEntropyLoss()\n    model.eval()  # Set the model to evaluation mode\n    model.to(device)\n    \n    running_loss = 0.0\n    correct = 0\n    total = 0\n    \n    with torch.no_grad():  # Disable gradient calculation during validation\n        for inputs, labels in dataloader:\n            inputs = [input_tensor.to(device) for input_tensor in inputs]\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            loss = criterion(outputs, labels.long())\n\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = 100 * correct / total\n    average_loss = running_loss / len(dataloader)\n    print(f'Validation Loss: {average_loss:.4f}, Validation Accuracy: {accuracy:.2f}%')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Usage:\nvalidate_model(model, dataloader_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Model Testing\ndef predict(model, x_test, device=\"mps\"):\n    model.eval()  # Set the model to evaluation mode\n    with torch.no_grad():  # Disable gradient calculation during inference\n        # Convert input data to torch tensor and move to device\n        inputs = [torch.tensor(sample, dtype=torch.float32).to(device) for sample in x_test]\n\n        # Forward pass\n        logits = model.forward(inputs)\n\n        # Convert logits to probabilities using softmax\n        probabilities = nn.Softmax(dim=1)(logits).cpu().numpy()\n\n        # Get predicted classes by taking argmax\n        predicted_classes = np.argmax(probabilities, axis=1)\n\n    return probabilities, predicted_classes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"probabilities, predicted_classes = predict(model, x_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(probabilities)\nprint(predicted_classes)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Report\n## Calculate\n### Classification Report\nclass_names={0: 'non_demented', 1: 'very_mild_demented', 2: 'mild_demented', 3: 'moderate_demented'}\n# Check to see if classes are all in the folder if not remove class from class_names list\nclass_names = [class_names[i] for i in np.unique(y_test)]\n# Print the classification report\n# Print the classification report\nprint(classification_report(y_test, predicted_classes, target_names=class_names))\n# Save the classification report\nwith open(results_folder + '/model_pretrained_classification_report.txt', 'w') as f:\n    with redirect_stdout(f):\n        print(classification_report(y_test, predicted_classes, target_names=class_names))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Confusion Matrix\n# Plot the confusion matrix\nconf_matrix = confusion_matrix(y_test, predicted_classes)\nfig, ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=class_names, yticklabels=class_names)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.savefig(results_folder + '/model_pretrained_confusion_matrix.png')  \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### ROC Curve and AUC\n\n# Convert y_test to one-hot for ROC calculation\ny_test_one_hot = tf.keras.utils.to_categorical(y_test, num_classes=len(class_names))\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(len(class_names)):\n    fpr[i], tpr[i], _ = roc_curve(y_test_one_hot[:, i], y_pred[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Save the ROC curve\nfig, ax = plt.subplots(figsize=(8, 8))\nfor i in range(len(class_names)):\n    plt.plot(fpr[i], tpr[i], label=f'{class_names[i]} (area = {roc_auc[i]:0.2f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig(results_folder + '/model_pretrained_roc_curve_for_all_classes.png')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot ROC curve for each class\nfor i in range(len(class_names)):\n    plt.figure()\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic for class {}'.format(class_names[i]))\n    plt.legend(loc=\"lower right\")\n    plt.savefig(results_folder + f'/model_pretrained_roc_curve_{class_names[i]}.png')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Precision-Recall curve\n# Compute Precision-Recall curve and area for each class\nprecision = dict()\nrecall = dict()\naverage_precision = dict()\nfor i in range(len(class_names)):\n    precision[i], recall[i], _ = precision_recall_curve(y_test_one_hot[:, i], predicted_classes[:, i])\n    average_precision[i] = average_precision_score(y_test_one_hot[:, i], predicted_classes[:, i])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display Precision-Recall curve for each class\nfig, ax = plt.subplots(figsize=(8, 8))\nfor i in range(len(class_names)):\n    plt.plot(recall[i], precision[i], label=f'{class_names[i]} ({average_precision[i]:0.2f})')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall curve for each class')\nplt.legend(loc=\"lower right\")\nplt.savefig(results_folder + '/model_pretrained_precision_recall_curve_for_all_classes.png')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Plot Precision-Recall curve for each class\nfor i in range(len(class_names)):\n    plt.figure()\n    plt.plot(recall[i], precision[i], label='Precision-Recall curve (area = %0.2f)' % average_precision[i])\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision-Recall curve for class {}'.format(class_names[i]))\n    plt.legend(loc=\"lower right\")\n    plt.savefig(results_folder + f'/model_pretrained_precision_recall_curve_{class_names[i]}.png')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Compute the MMCE\n# Compute the MMCE\n    # implement the test accuracty from y_test and predicted_classes\ntest_accuracy = accuracy_score(y_test, predicted_classes)\nmmce = 1 - test_accuracy\nprint(f'MMCE: {mmce:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Logarithmic Loss\nprint(y_test_one_hot.shape)\nprint(predicted_classes[:,:3].shape)\n# Create a CategoricalCrossentropy instance\nloss_fn = tf.keras.losses.CategoricalCrossentropy()\n\n# Compute the log loss\nlog_loss = loss_fn(y_test_one_hot[:,:3], predicted_classes[:,:3]).numpy()  # Use .numpy() to convert to a scalar\n\nprint(f'Log loss: {log_loss:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Mean Absolute Error\n# Compute the mean absolute error\nmae = tf.keras.losses.mean_absolute_error(y_test_one_hot[:,:3], predicted_classes[:,:3])\nmean_mae = tf.reduce_mean(mae)  # Calculate the mean of the MAE values\n\nprint(f'Mean absolute error: {mean_mae:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Mean Squared Error\n# Compute the mean squared error\nmse = tf.keras.losses.mean_squared_error(y_test_one_hot[:,:3], predicted_classes[:,:3])\nmean_mse = tf.reduce_mean(mse)  # Calculate the mean of the MAE values\n\nprint(f'Mean absolute error: {mean_mse:.4f}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### F1 Score\nprint(y_test.dtype)\nprint(predicted_classes.dtype)\n# Compute F1 score\nf1score = f1_score(y_test, predicted_classes, average='weighted')\n# compute the test loss from the y_test and predicted_classes\ntest_loss = loss_fn(y_test_one_hot[:,:3], predicted_classes[:,:3]).numpy()\n\nprint(f'F1 score: {f1score:.4f}')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Save\n# Save all the results in a txt file\nwith open(results_folder + '/model_pretrained_results.txt', 'w') as f:\n    with redirect_stdout(f):\n        print('Test loss:', test_loss)\n        print('Test accuracy:', test_accuracy)\n        print(classification_report(y_test, predicted_classes, target_names=class_names))\n        print('MMCE:', mmce)\n        print('Log loss:', log_loss)\n        print('Mean absolute error:', mean_mae)\n        print('Mean squared error:', mean_mse)\n        print('F1 score:', f1_score)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}